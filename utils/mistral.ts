import { Mistral } from "@mistralai/mistralai"; // Import the Mistral client library to interact with Mistral AI
import { Document } from "langchain/document"; // Import the Document type from LangChain, used for representing text data
import { MistralAIEmbeddings } from "@langchain/mistralai"; // Import the Mistral AI embeddings class from LangChain
import { MistralMessage } from "@/lib/interface"; // Import the MistralMessage type, used for structuring chat messages
import { EventStream } from "@mistralai/mistralai/lib/event-streams"; // Import the EventStream type, used for handling chat responses as streams
import {
  ChatCompletionResponse,
  CompletionEvent,
  EmbeddingResponse,
} from "@mistralai/mistralai/models/components"; // Import CompletionEvent, representing each step in the response stream
import { ChatMistralAI } from "@langchain/mistralai";
import { Tool } from "@mistralai/mistralai/models/components";

// Retrieve the Mistral API key from environment variables
const apiKey = process.env.MISTRAL_API_KEY;

// Define the Mistral embedding model name (this is the model that generates text embeddings)
export const mistralEmbeddingModelName = "mistral-embed";
export const mistralChatModelName = "mistral-large-latest";

// Initialize the Mistral client with the API key (this is used to interact with Mistral's APIs)
export const mistralClient = new Mistral({ apiKey: apiKey });

export const mistralLangchainClient = new ChatMistralAI({
  model: mistralChatModelName,
  temperature: 0.7,
  maxRetries: 2,
  apiKey,
});
/**
 * Function to create embeddings for text chunks using Mistral's embedding model.
 * Embeddings are numerical representations of text that capture the semantic meaning of the text.
 *
 * @param chunks - An array of `Document` objects or a string. Each document contains text content that will be embedded.
 * @returns A promise that resolves to the embeddings generated by Mistral's API.
 */
export const createMistralEmbeddingUsingDocumentsAndString = async (
  chunks: Document[] | string
): Promise<EmbeddingResponse> => {
  return mistralClient.embeddings.create({
    model: mistralEmbeddingModelName, // Specify the Mistral embedding model to use
    inputs:
      typeof chunks !== "string"
        ? chunks.map((chunk) => chunk.pageContent) // Extract the text content from each Document object if chunks is an array
        : chunks, // If chunks is a string, just pass it as is
  });
};

/**
 * Mistral Embedding Instance: This is an instance of the `MistralAIEmbeddings` class,
 * which provides the embeddings functionality for the Mistral model.
 * It can be used to create embeddings in a more object-oriented way.
 */
export const minstralEmbedingsInstance = new MistralAIEmbeddings({
  model: mistralEmbeddingModelName, // Specify the model to use for embeddings (default is `mistral-embed`)
});

/**
 * Function to stream a chat response from the Mistral AI model.
 * This function streams responses in real-time, which is ideal for long or ongoing interactions.
 *
 * @param messages - An array of `MistralMessage` objects, where each object represents a message with a `role` (system, user, or assistant) and `content`.
 * @param temperature - A number that controls the randomness in the model's response (default is 0.7).
 * @returns An `EventStream` of `CompletionEvent`, which can be consumed to get the chat response incrementally.
 */
export async function mistralChatStreamResponse(
  messages: MistralMessage[], // Array of messages exchanged in the conversation
  temperature: number = 0.7 // Controls the creativity of the AI's response (higher values = more randomness)
): Promise<EventStream<CompletionEvent>> {
  try {
    // Call the Mistral chat stream API, passing in the messages and the model configuration
    const chatResponse = await mistralClient.chat.stream({
      model: mistralChatModelName, // The specific model to use for chat responses
      // Check if there is a system message. If not, add a default system message
      messages: messages.some((m) => m.role === "system")
        ? messages // If a system message is already present, use the provided messages
        : [
            {
              role: "system", // System role is used to define the behavior of the assistant
              content:
                "You are a friendly cheese connoisseur. When asked about cheese, reply concisely and humorously.",
            },
            ...messages, // Add the provided messages after the system message
          ],
      temperature: temperature, // Pass the temperature to control the randomness
      responseFormat: {
        type: "text", // Define the response format as plain text
      },
    });

    return chatResponse; // Return the chat response as an event stream
  } catch (error) {
    // Handle any errors that occur during the API call
    console.error("Error fetching chat response:", error);
    throw new Error("Failed to get chat response");
  }
}

/**
 * Function to fetch a complete chat response from the Mistral AI model.
 * This function sends a series of messages to the model and returns a full response once processed.
 *
 * @param messages - An array of `MistralMessage` objects, where each message represents a conversation step.
 * @param temperature - A number that controls the creativity of the AI's response. Higher values introduce more randomness (default is 0.7).
 * @returns A promise that resolves to the complete chat response from the Mistral model.
 */
export async function mistralChatCompleteResponse(
  messages: MistralMessage[], // Array of messages exchanged in the conversation
  tools: Tool[] = [],
  temperature: number = 0.7 // Controls the creativity of the AI's response (higher values = more randomness)
): Promise<ChatCompletionResponse> {
  try {
    // Call the Mistral chat complete API with the provided messages and model configuration
    const chatResponse = await mistralClient.chat.complete({
      model: mistralChatModelName, // Specify the model to use for chat responses
      // Check if a system message is provided; if not, add a default one
      messages: messages.some((m) => m.role === "system") // If there's already a system message, use the given messages
        ? messages
        : [
            {
              role: "system", // System message defines the assistant's behavior
              content:
                "You are a friendly cheese connoisseur. When asked about cheese, reply concisely and humorously.",
            },
            ...messages, // Append the user-provided messages after the system message
          ],
      tools,
      temperature: temperature, // Pass the temperature for controlling randomness
      responseFormat: {
        type: "text", // The response will be in plain text format
      },
    });

    return chatResponse; // Return the final chat response
  } catch (error) {
    // Handle any errors that occur during the API call
    console.error("Error fetching chat response:", error);
    throw new Error("Failed to get chat response"); // Throw a new error if the chat response fails
  }
}
